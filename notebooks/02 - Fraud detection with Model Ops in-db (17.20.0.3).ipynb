{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5184d6-a876-4932-98ee-32eed022d2f1",
   "metadata": {},
   "source": [
    "![](./images/Title.PNG)\n",
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45ac9b-ff6d-4494-8a2a-58cc144af4d7",
   "metadata": {},
   "source": [
    "# Fraud detection from transactions\n",
    "![](./images/workflow_fraud.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14a2ac-c887-4cc2-8d14-59c02a1fb9ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce617147-6c1c-49d7-a689-23ae15689fa0",
   "metadata": {},
   "source": [
    "## 1 - Connect to Vantage\n",
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191c5f8-65a5-454b-bbda-27a5e06dbab2",
   "metadata": {},
   "source": [
    "![](./images/Slide32.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48be49cf-c99f-49a0-9b6e-65dca5082f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import teradataml as tdml\n",
    "tdml.options.configure.byom_install_location = \"mldb\"\n",
    "tdml.display.print_sqlmr_query = False\n",
    "import getpass\n",
    "import json\n",
    "tdml.__version__\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "tdml.options.display\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82424a5-65fb-4795-9c2e-427e59b18ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17.20.00.04'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bcb716-c7a5-4c54-985c-5104a289a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9266811-6f73-4758-acb3-bfa40e558a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the local project repository for this model demo\n",
    "model_local_path = 'C:/Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb'\n",
    "res = os.system(f'mkdir -p \"{model_local_path}\"')\n",
    "res = os.system(f'mkdir -p \"{model_local_path}/model_modules\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30baadf2-d62d-4ff4-9a88-595db218ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dm250067\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\context\\context.py:480: TeradataMlRuntimeWarning: Warning: Password is URL encoded.\n",
      "  warnings.warn(\"Warning: Password is URL encoded.\", category=TeradataMlRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(teradatasql://:***@tdprd3.td.teradata.com/?DATABASE=ADLDSD_CHURN&LOGDATA=%2A%2A%2A&LOGMECH=%2A%2A%2A&USER=DM250067)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Param = {\n",
    "    'host'          : 'tdprd2.td.teradata.com', \n",
    "    'user'          : 'dm250067', \n",
    "    'password'      : \"ENCRYPTED_PASSWORD(file:{},file:{})\".format ('../../PassKey.properties','../../EncPass.properties'), #getpass.getpass(), \n",
    "    'logmech'       : 'LDAP',\n",
    "    'database'      : 'ADLSLSEMEA_DEMO_BANKING',\n",
    "    'temp_database_name' : 'dm250067'\n",
    "    }\n",
    "\n",
    "Param = {\n",
    "    'host'          : 'tdprd3.td.teradata.com', \n",
    "    'user'          : 'dm250067', \n",
    "    'password'      : \"ENCRYPTED_PASSWORD(file:{},file:{})\".format ('../../PassKey.properties','../../EncPass.properties'), #getpass.getpass(), \n",
    "    'logmech'       : 'LDAP',\n",
    "    'database'      : 'ADLDSD_CHURN',\n",
    "    'temp_database_name' : 'dm250067'\n",
    "    }\n",
    "\n",
    "tdml.create_context(**Param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4c1a8-1ee3-4ea8-a3fa-1f0c3e0809cb",
   "metadata": {},
   "source": [
    "## 2 - Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af36fee-cce8-4a53-97ef-a9a985726c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20cccfd-b426-4f17-aa6e-2a3701b3e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb/model_modules/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_local_path/model_modules/training.py\"\n",
    "from teradataml import (\n",
    "    DataFrame,\n",
    "    OneHotEncodingFit,\n",
    "    OneHotEncodingTransform,\n",
    "    ScaleFit,\n",
    "    ScaleTransform,\n",
    "    DecisionForest,\n",
    "    configure\n",
    ")\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "configure.val_install_location = 'TRNG_XSP'\n",
    "\n",
    "def plot_roc_curve(fi, img_filename):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    feat_importances = pd.Series(fi)\n",
    "    feat_importances.nlargest(10).plot(kind='barh').set_title('Feature Importance')\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name   = context.dataset_info.target_names[0]\n",
    "    entity_key    = context.dataset_info.entity_key\n",
    "\n",
    "    # read training dataset from Teradata and convert to pandas\n",
    "    train_df      = DataFrame.from_query(context.dataset_info.sql)\n",
    "    \n",
    "    print('feature names :', feature_names)\n",
    "    \n",
    "    if 'transaction_type' in feature_names:\n",
    "        print (\"OneHotEncoding using InDB Functions...\")\n",
    "        \n",
    "        transaction_types = list(train_df[['transaction_type','txn_id']].groupby(['transaction_type']).count().to_pandas()['transaction_type'].values)\n",
    "\n",
    "\n",
    "        onehot = OneHotEncodingFit(data           = train_df,\n",
    "                                        is_input_dense  = True,\n",
    "                                        target_column      = 'transaction_type',\n",
    "                                        categorical_values = transaction_types,\n",
    "                                        other_column=\"other\"\n",
    "                                       )\n",
    "\n",
    "        train_df_onehot = OneHotEncodingTransform(data=train_df,\n",
    "                                           object=onehot.result,\n",
    "                                           is_input_dense=True\n",
    "                                          ).result\n",
    "\n",
    "        onehot.result.to_sql(f\"onehot_${context.model_version}\", if_exists=\"replace\")\n",
    "        print(\"Saved onehot\")\n",
    "        \n",
    "        feature_names_after_one_hot = [c for c in feature_names if c != 'transaction_type'] + ['transaction_type_'+c for c in transaction_types]\n",
    "        category_features = ['transaction_type']\n",
    "    else:\n",
    "        train_df_onehot = train_df\n",
    "        feature_names_after_one_hot = feature_names\n",
    "        category_features = []\n",
    "    \n",
    "    print (\"Scaling using InDB Functions...\")\n",
    "    print(feature_names_after_one_hot)\n",
    "    \n",
    "    scaler = ScaleFit(\n",
    "        data           = train_df_onehot,\n",
    "        target_columns = feature_names_after_one_hot,\n",
    "        scale_method   = context.hyperparams[\"scale_method\"],\n",
    "        miss_value     = context.hyperparams[\"miss_value\"],\n",
    "        global_scale   = context.hyperparams[\"global_scale\"].lower() in ['true', '1'],\n",
    "        multiplier     = context.hyperparams[\"multiplier\"],\n",
    "        intercept      = context.hyperparams[\"intercept\"]\n",
    "    )\n",
    "\n",
    "    scaled_train = ScaleTransform(\n",
    "        data           = train_df_onehot,\n",
    "        object         = scaler.output,\n",
    "        accumulate     = [target_name, entity_key]\n",
    "    ).result\n",
    "    \n",
    "    scaler.output.to_sql(f\"scaler_${context.model_version}\", if_exists=\"replace\")\n",
    "    print(\"Saved scaler\")\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    model = DecisionForest(\n",
    "        input_columns        = feature_names_after_one_hot,\n",
    "        response_column      = target_name,\n",
    "        data                 = scaled_train,\n",
    "        max_depth            = context.hyperparams[\"max_depth\"],\n",
    "        num_trees            = context.hyperparams[\"num_trees\"],\n",
    "        min_node_size        = context.hyperparams[\"min_node_size\"],\n",
    "        mtry                 = context.hyperparams[\"mtry\"],\n",
    "        mtry_seed            = context.hyperparams[\"mtry_seed\"],\n",
    "        seed                 = context.hyperparams[\"seed\"],\n",
    "        tree_type            = 'CLASSIFICATION'\n",
    "    )\n",
    "    \n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")    \n",
    "    print(\"Saved trained model\")\n",
    "\n",
    "    record_training_stats(\n",
    "        train_df,\n",
    "        features    = feature_names,\n",
    "        targets     = [target_name],\n",
    "        categorical = [target_name]+category_features,\n",
    "        #feature_importance = {f:0 for f in feature_names_after_one_hot},\n",
    "        context     = context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3740ac-b6f1-4c07-8325-a340b1249efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names : ['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'transaction_type']\n",
      "OneHotEncoding using InDB Functions...\n",
      "Saved onehot\n",
      "Scaling using InDB Functions...\n",
      "['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'transaction_type_CASH_IN', 'transaction_type_CASH_OUT', 'transaction_type_TRANSFER', 'transaction_type_PAYMENT', 'transaction_type_DEBIT']\n",
      "Saved scaler\n",
      "Starting training...\n",
      "Saved trained model\n"
     ]
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the training dataset \n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM {Param['database']}.transactions_fraud\n",
    "where fold = 'train'\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": Param['database'],\n",
    "    \"table\": \"transactions_features\"\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    # scaler\n",
    "    \"scale_method\":\"STD\",\n",
    "    \"miss_value\":\"KEEP\",\n",
    "    \"global_scale\":\"False\",\n",
    "    \"multiplier\":\"1\",\n",
    "    \"intercept\":\"0\",\n",
    "    # decision forest\n",
    "    \"max_depth\": 15, \n",
    "    \"num_trees\": 72,\n",
    "    \"min_node_size\": 1,\n",
    "    \"mtry\": 6,\n",
    "    \"mtry_seed\": 1,\n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "entity_key    = \"txn_id\"\n",
    "target_names  = [\"isFraud\"]\n",
    "feature_names = ['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'transaction_type']\n",
    "\n",
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "dataset_info = DatasetInfo(\n",
    "    sql=sql,\n",
    "    entity_key=entity_key,\n",
    "    feature_names=feature_names,\n",
    "    target_names=target_names,\n",
    "    feature_metadata=feature_metadata\n",
    ")\n",
    "\n",
    "ctx = ModelContext(\n",
    "    hyperparams=hyperparams,\n",
    "    dataset_info=dataset_info,\n",
    "    artifact_output_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "    model_version=\"InDB_v1\",\n",
    "    model_table=\"aoa_model_indb_v1\"\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.expanduser(f\"{model_local_path}/model_modules\"))\n",
    "import training\n",
    "training.train(context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51dcb38-3300-4b80-a40f-8b2d08d4b452",
   "metadata": {},
   "source": [
    "## 3 - Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca24555-8743-45d0-8b71-edfc19140578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb/model_modules/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_local_path/model_modules/evaluation.py\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from teradataml import (\n",
    "    copy_to_sql,\n",
    "    DataFrame,\n",
    "    OneHotEncodingTransform,\n",
    "    DecisionForestPredict,\n",
    "    TDGLMPredict,\n",
    "    ScaleTransform,\n",
    "    ClassificationEvaluator,\n",
    "    ConvertTo,\n",
    "    ROC\n",
    ")\n",
    "from aoa import (\n",
    "    record_evaluation_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_feature_importance(fi, img_filename):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    feat_importances = pd.Series(fi)\n",
    "    feat_importances.nlargest(10).plot(kind='barh').set_title('Feature Importance')\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cf, img_filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(cf, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(cf.shape[0]):\n",
    "        for j in range(cf.shape[1]):\n",
    "            ax.text(x=j, y=i,s=cf[i, j], va='center', ha='center', size='xx-large')\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix');\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "def plot_roc_curve(roc_out, img_filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    auc = roc_out.result.to_pandas().reset_index()['AUC'][0]\n",
    "    roc_results = roc_out.output_data.to_pandas()\n",
    "    plt.plot(roc_results['fpr'], roc_results['tpr'], color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % 0.27)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    test_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    \n",
    "    # One hot encoding\n",
    "    if 'transaction_type' in feature_names:\n",
    "        print (\"Loading onehot...\")\n",
    "        onehot = DataFrame(f\"onehot_${context.model_version}\")\n",
    "        onehot_test = OneHotEncodingTransform(data=test_df,\n",
    "                              object=onehot,\n",
    "                              is_input_dense=True).result        \n",
    "    else:\n",
    "        print (\"no onehotencoding\")\n",
    "        onehot_test = test_df\n",
    "    \n",
    "\n",
    "    # Scaling the test set\n",
    "    print (\"Loading scaler...\")\n",
    "    scaler = DataFrame(f\"scaler_${context.model_version}\")\n",
    "\n",
    "    scaled_test = ScaleTransform(\n",
    "        data=onehot_test,\n",
    "        object=scaler,\n",
    "        accumulate = [target_name,entity_key]\n",
    "    ).result\n",
    "    \n",
    "    print(\"Scoring\")\n",
    "    predictions = DecisionForestPredict(object = model,\n",
    "                                                newdata = scaled_test,\n",
    "                                                id_column = entity_key,                                                        \n",
    "                                                terms = [target_name],\n",
    "                                                accumulate = [target_name],\n",
    "                                                output_prob = True,\n",
    "                                                output_responses=[\"0\",\"1\"]\n",
    "                                                )\n",
    "    print(predictions)\n",
    "\n",
    "    predicted_data = ConvertTo(\n",
    "        data = predictions.result,\n",
    "        target_columns = [target_name,'prediction'],\n",
    "        target_datatype = [\"INTEGER\"]\n",
    "    )\n",
    "\n",
    "    ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "        data=predicted_data.result,\n",
    "        observation_column=target_name,\n",
    "        prediction_column='prediction',\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "    metrics_pd = ClassificationEvaluator_obj.output_data.to_pandas()\n",
    "\n",
    "    evaluation = {\n",
    "        'Accuracy': '{:.2f}'.format(metrics_pd.MetricValue[0]),\n",
    "        'Micro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[1]),\n",
    "        'Micro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[2]),\n",
    "        'Micro-F1': '{:.2f}'.format(metrics_pd.MetricValue[3]),\n",
    "        'Macro-Precision': '{:.2f}'.format(metrics_pd.MetricValue[4]),\n",
    "        'Macro-Recall': '{:.2f}'.format(metrics_pd.MetricValue[5]),\n",
    "        'Macro-F1': '{:.2f}'.format(metrics_pd.MetricValue[6]),\n",
    "        'Weighted-Precision': '{:.2f}'.format(metrics_pd.MetricValue[7]),\n",
    "        'Weighted-Recall': '{:.2f}'.format(metrics_pd.MetricValue[8]),\n",
    "        'Weighted-F1': '{:.2f}'.format(metrics_pd.MetricValue[9]),\n",
    "    }\n",
    "\n",
    "    with open(f\"{context.artifact_output_path}/metrics.json\", \"w+\") as f:\n",
    "        json.dump(evaluation, f)\n",
    "        \n",
    "    cm = confusion_matrix(predicted_data.result.to_pandas()['isFraud'], predicted_data.result.to_pandas()['prediction'])\n",
    "    plot_confusion_matrix(cm, f\"{context.artifact_output_path}/confusion_matrix\")\n",
    "\n",
    "    roc_out = ROC(\n",
    "        data=predictions.result,\n",
    "        probability_column='prob',\n",
    "        observation_column=target_name,\n",
    "        positive_class='1',\n",
    "        num_thresholds=1000\n",
    "    )\n",
    "    plot_roc_curve(roc_out, f\"{context.artifact_output_path}/roc_curve\")\n",
    "\n",
    "    predictions_table = \"predictions_tmp\"\n",
    "    copy_to_sql(df=predicted_data.result, table_name=predictions_table, index=False, if_exists=\"replace\", temporary=True)\n",
    "\n",
    "    # calculate stats if training stats exist\n",
    "    if os.path.exists(f\"{context.artifact_input_path}/data_stats.json\"):\n",
    "        record_evaluation_stats(\n",
    "            features_df=test_df,\n",
    "            predicted_df=DataFrame.from_query(f\"SELECT * FROM {predictions_table}\"),\n",
    "            #feature_importance=feature_importance,\n",
    "            context=context\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3199e8d2-e803-4972-8c88-14c647d6ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading onehot...\n",
      "Loading scaler...\n",
      "Scoring\n"
     ]
    },
    {
     "ename": "TeradataMlException",
     "evalue": "[Teradata][teradataml](TDML_2001) Following required arguments are missing: ['newdata'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTeradataMlException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m\n\u001b[0;32m     24\u001b[0m ctx \u001b[38;5;241m=\u001b[39m ModelContext(\n\u001b[0;32m     25\u001b[0m     hyperparams\u001b[38;5;241m=\u001b[39mhyperparams,\n\u001b[0;32m     26\u001b[0m     dataset_info\u001b[38;5;241m=\u001b[39mdataset_info,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     model_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maoa_model_indb_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#evaluate(context=ctx)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# view evaluation results\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb/model_modules\\evaluation.py:101\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(context, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m scaled_test \u001b[38;5;241m=\u001b[39m ScaleTransform(\n\u001b[0;32m     95\u001b[0m     data\u001b[38;5;241m=\u001b[39monehot_test,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39mscaler,\n\u001b[0;32m     97\u001b[0m     accumulate \u001b[38;5;241m=\u001b[39m [target_name,entity_key]\n\u001b[0;32m     98\u001b[0m )\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScoring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionForestPredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaled_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mentity_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                                        \u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mterms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43maccumulate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43moutput_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43moutput_responses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m    111\u001b[0m predicted_data \u001b[38;5;241m=\u001b[39m ConvertTo(\n\u001b[0;32m    112\u001b[0m     data \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mresult,\n\u001b[0;32m    113\u001b[0m     target_columns \u001b[38;5;241m=\u001b[39m [target_name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    114\u001b[0m     target_datatype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTEGER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    115\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\analytics\\sqle\\__init__.py:106\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m _sqle_functions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANOVA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAntiselect\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttribution\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZTest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    103\u001b[0m                   ]\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _sqle_functions:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[func] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(func), (_AnalyticFunction, ), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _common_init(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqle\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    107\u001b[0m                                                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m\"\u001b[39m: _AnalyticFunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m})\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\analytics\\meta_class.py:188\u001b[0m, in \u001b[0;36m_common_init\u001b[1;34m(self, function_type, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mteradataml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytic_function_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SQLEFunctionExecutor\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m _SQLEFunctionExecutor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;241m.\u001b[39m_execute_function(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m function_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muaf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mteradataml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytic_function_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _UAFFunctionExecutor\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\analytics\\analytic_function_executor.py:697\u001b[0m, in \u001b[0;36m_AnlyticFunctionExecutor._execute_function\u001b[1;34m(self, skip_input_arg_processing, skip_output_arg_processing, skip_other_arg_processing, skip_func_output_processing, skip_dyn_cls_processing, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dyn_cls_data_members\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_input_arg_processing:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_input_argument(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_output_arg_processing:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_output_argument(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\analytics\\analytic_function_executor.py:910\u001b[0m, in \u001b[0;36m_SQLEFunctionExecutor._process_input_argument\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    907\u001b[0m     reference_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mget_reference_function_class()\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# Validate the input table arguments.\u001b[39;00m\n\u001b[1;32m--> 910\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_analytic_function_argument\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_table_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_table_arg_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_valid_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# If input is an object of reference Function, then get the DataFrame from it.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reference_class \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_table_arg_value, reference_class):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\analytics\\analytic_function_executor.py:138\u001b[0m, in \u001b[0;36m_AnlyticFunctionExecutor._validate_analytic_function_argument\u001b[1;34m(func_arg_name, func_arg_value, argument, additional_valid_types)\u001b[0m\n\u001b[0;32m    131\u001b[0m         py_types \u001b[38;5;241m=\u001b[39m (py_types, additional_valid_types)\n\u001b[0;32m    133\u001b[0m argument_info \u001b[38;5;241m=\u001b[39m [func_arg_name,\n\u001b[0;32m    134\u001b[0m                  func_arg_value,\n\u001b[0;32m    135\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m argument\u001b[38;5;241m.\u001b[39mis_required(),\n\u001b[0;32m    136\u001b[0m                  py_types\n\u001b[0;32m    137\u001b[0m                  ]\n\u001b[1;32m--> 138\u001b[0m \u001b[43m_Validators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_missing_required_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43margument_info\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Validate for empty string if argument accepts a column name for either input or output.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m argument\u001b[38;5;241m.\u001b[39mis_empty_value_allowed() \u001b[38;5;129;01mor\u001b[39;00m argument\u001b[38;5;241m.\u001b[39mis_output_column():\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\utils\\validators.py:591\u001b[0m, in \u001b[0;36m_Validators._validate_missing_required_arguments\u001b[1;34m(arg_list)\u001b[0m\n\u001b[0;32m    588\u001b[0m         miss_args\u001b[38;5;241m.\u001b[39mappend(args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(miss_args)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TeradataMlException(Messages\u001b[38;5;241m.\u001b[39mget_message(MessageCodes\u001b[38;5;241m.\u001b[39mMISSING_ARGS,miss_args),\n\u001b[0;32m    592\u001b[0m                               MessageCodes\u001b[38;5;241m.\u001b[39mMISSING_ARGS)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mTeradataMlException\u001b[0m: [Teradata][teradataml](TDML_2001) Following required arguments are missing: ['newdata']."
     ]
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the evaluation dataset \n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM {Param['database']}.transactions_fraud\n",
    "where fold = 'test'\n",
    "\"\"\"\n",
    "\n",
    "entity_key    = \"txn_id\"\n",
    "target_names  = [\"isFraud\"]\n",
    "feature_names = ['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'transaction_type']\n",
    " \n",
    "dataset_info = DatasetInfo(\n",
    "    sql=sql,\n",
    "    entity_key=entity_key,\n",
    "    feature_names=feature_names,\n",
    "    target_names=target_names,\n",
    "    feature_metadata=feature_metadata\n",
    ")\n",
    "\n",
    "ctx = ModelContext(\n",
    "    hyperparams=hyperparams,\n",
    "    dataset_info=dataset_info,\n",
    "    artifact_output_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "    artifact_input_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "    model_version=\"InDB_v1\",\n",
    "    model_table=\"aoa_model_indb_v1\"\n",
    ")\n",
    "\n",
    "import evaluation\n",
    "evaluation.evaluate(context=ctx)\n",
    "#evaluate(context=ctx)\n",
    "\n",
    "# view evaluation results\n",
    "import json\n",
    "with open(f\"{ctx.artifact_output_path}/metrics.json\") as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c894bf-bcaa-4c10-9ba0-220c3f42593f",
   "metadata": {},
   "source": [
    "## 4 - Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bf066-8722-4029-9b5f-90a5df3b21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_local_path/model_modules/scoring.py\"\n",
    "from teradataml import (\n",
    "    copy_to_sql,\n",
    "    DataFrame,\n",
    "    TDGLMPredict,\n",
    "    ScaleTransform,\n",
    "    OneHotEncodingTransform,\n",
    "    TDDecisionForestPredict\n",
    ")\n",
    "from aoa import (\n",
    "    record_scoring_stats,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def score(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    model = DataFrame(f\"model_${context.model_version}\")\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    features_tdf = DataFrame.from_query(context.dataset_info.sql)\n",
    "    features_pdf = features_tdf.to_pandas(all_rows=True)\n",
    "\n",
    "    # One hot encoding\n",
    "    if 'transaction_type' in feature_names:\n",
    "        print (\"Loading onehot...\")\n",
    "        onehot = DataFrame(f\"onehot_${context.model_version}\")\n",
    "        onehot_test = OneHotEncodingTransform(data=features_tdf,\n",
    "                              object=onehot,\n",
    "                              is_input_dense=True).result        \n",
    "    else:\n",
    "        print (\"no onehotencoding\")\n",
    "        onehot_test = features_tdf\n",
    "    \n",
    "    # Scaling the scoring set\n",
    "    print (\"Loading scaler...\")\n",
    "    scaler = DataFrame(f\"scaler_${context.model_version}\")\n",
    "\n",
    "    scaled_features = ScaleTransform(\n",
    "        data=onehot_test,\n",
    "        object=scaler,\n",
    "        accumulate = entity_key\n",
    "    )\n",
    "    \n",
    "    print(\"Scoring\")\n",
    "    predictions = TDDecisionForestPredict(\n",
    "        object=model,\n",
    "        data=scaled_features.result,\n",
    "        id_column=entity_key\n",
    "    )\n",
    "\n",
    "    predictions_pdf = predictions.result.to_pandas(all_rows=True).rename(columns={\"prediction\": target_name}).astype(int)\n",
    "\n",
    "    print(\"Finished Scoring\")\n",
    "\n",
    "    # store the predictions\n",
    "    predictions_pdf = pd.DataFrame(predictions_pdf, columns=[target_name])\n",
    "    predictions_pdf[entity_key] = features_pdf.index.values\n",
    "    # add job_id column so we know which execution this is from if appended to predictions table\n",
    "    predictions_pdf[\"job_id\"] = context.job_id\n",
    "\n",
    "    # teradataml doesn't match column names on append.. and so to match / use same table schema as for byom predict\n",
    "    # example (see README.md), we must add empty json_report column and change column order manually (v17.0.0.4)\n",
    "    # CREATE MULTISET TABLE pima_patient_predictions\n",
    "    # (\n",
    "    #     job_id VARCHAR(255), -- comes from airflow on job execution\n",
    "    #     PatientId BIGINT,    -- entity key as it is in the source data\n",
    "    #     HasDiabetes BIGINT,   -- if model automatically extracts target\n",
    "    #     json_report CLOB(1048544000) CHARACTER SET UNICODE  -- output of\n",
    "    # )\n",
    "    # PRIMARY INDEX ( job_id );\n",
    "    predictions_pdf[\"json_report\"] = \"\"\n",
    "    predictions_pdf = predictions_pdf[[\"job_id\", entity_key, target_name, \"json_report\"]]\n",
    "\n",
    "    copy_to_sql(\n",
    "        df=predictions_pdf,\n",
    "        schema_name=context.dataset_info.predictions_database,\n",
    "        table_name=context.dataset_info.predictions_table,\n",
    "        index=False,\n",
    "        if_exists=\"append\"\n",
    "    )\n",
    "    \n",
    "    print(\"Saved predictions in Teradata\")\n",
    "\n",
    "    # calculate stats\n",
    "    predictions_df = DataFrame.from_query(f\"\"\"\n",
    "        SELECT \n",
    "            * \n",
    "        FROM {context.dataset_info.get_predictions_metadata_fqtn()} \n",
    "            WHERE job_id = '{context.job_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    record_scoring_stats(features_df=features_tdf, predicted_df=predictions_df, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323664a-f40a-47ed-888f-710fa1dcc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the scoring dataset \n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT TOP 100\n",
    "*\n",
    "FROM {Param['database']}.transactions_fraud\n",
    "where fold = 'test'\n",
    "\"\"\"\n",
    "\n",
    "# where to store predictions\n",
    "predictions = {\n",
    "    \"database\": Param['database'],\n",
    "    \"table\": \"Transaction_fraud_predictions\"\n",
    "}\n",
    "\n",
    "import uuid\n",
    "job_id=str(uuid.uuid4())\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata,\n",
    "                           predictions=predictions)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "                   artifact_input_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "                   model_version=\"InDB_v1\",\n",
    "                   model_table=\"aoa_model_indb_v1\",\n",
    "                   job_id=job_id)\n",
    "\n",
    "import scoring\n",
    "scoring.score(context=ctx)\n",
    "score(context=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa4d25-5a95-4d44-b081-192b38a488a6",
   "metadata": {},
   "source": [
    "## 5 - Define model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cf316-db0b-4255-a3c3-1dcfad5b41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_local_path/model_modules/requirements.txt\"\n",
    "teradataml==17.20.0.3\n",
    "aoa==7.0.1\n",
    "pandas==1.1.5\n",
    "scikit-learn==0.24.2\n",
    "matplotlib==3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459724d-c517-4fe7-8e8c-afddc8bca259",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_local_path/config.json\"\n",
    "{\n",
    "   \"hyperParameters\": {\n",
    "        \"scale_method\":\"STD\",\n",
    "        \"miss_value\":\"KEEP\",\n",
    "        \"global_scale\":\"False\",\n",
    "        \"multiplier\":\"1\",\n",
    "        \"intercept\":\"0\",\n",
    "        \"max_depth\": 15, \n",
    "        \"num_trees\": 72,\n",
    "        \"min_node_size\": 1,\n",
    "        \"mtry\": 6,\n",
    "        \"mtry_seed\": 1,\n",
    "        \"seed\": 1\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c17b68-aaee-4e5b-b4a3-eb74f77b3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_local_path/model.json\"\n",
    "{\n",
    "    \"id\": \"f8df0bec-12d1-4d2d-920f-4448503df82e\",\n",
    "    \"name\": \"Python Fraud Transaction InDB DecisionForest\",\n",
    "    \"description\": \"Python InDB DecisionForest for Fraud Transaction prediction\",\n",
    "    \"language\": \"python\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f2264-8799-4eae-9df3-75a6d8da4358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
