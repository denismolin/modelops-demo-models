{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5184d6-a876-4932-98ee-32eed022d2f1",
   "metadata": {},
   "source": [
    "![](./images/Title.PNG)\n",
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45ac9b-ff6d-4494-8a2a-58cc144af4d7",
   "metadata": {},
   "source": [
    "# Fraud detection from transactions\n",
    "![](./images/workflow_fraud.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14a2ac-c887-4cc2-8d14-59c02a1fb9ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce617147-6c1c-49d7-a689-23ae15689fa0",
   "metadata": {},
   "source": [
    "## 1 - Connect to Vantage\n",
    "<div class=\"alert alert-block alert-info\"> <b> </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191c5f8-65a5-454b-bbda-27a5e06dbab2",
   "metadata": {},
   "source": [
    "![](./images/Slide32.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48be49cf-c99f-49a0-9b6e-65dca5082f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import teradataml as tdml\n",
    "tdml.options.configure.byom_install_location = \"mldb\"\n",
    "tdml.display.print_sqlmr_query = False\n",
    "import getpass\n",
    "import json\n",
    "tdml.__version__\n",
    "from datetime import datetime\n",
    "tic = datetime.now()\n",
    "tdml.options.display\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82424a5-65fb-4795-9c2e-427e59b18ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17.20.00.04'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bcb716-c7a5-4c54-985c-5104a289a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9266811-6f73-4758-acb3-bfa40e558a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the local project repository for this model demo\n",
    "model_local_path = 'C:/Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb'\n",
    "res = os.system(f'mkdir -p \"{model_local_path}\"')\n",
    "res = os.system(f'mkdir -p \"{model_local_path}/model_modules\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30baadf2-d62d-4ff4-9a88-595db218ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dm250067\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\teradataml\\context\\context.py:480: TeradataMlRuntimeWarning: Warning: Password is URL encoded.\n",
      "  warnings.warn(\"Warning: Password is URL encoded.\", category=TeradataMlRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(teradatasql://:***@tdprd3.td.teradata.com/?DATABASE=ADLDSD_CHURN&LOGDATA=%2A%2A%2A&LOGMECH=%2A%2A%2A&USER=DM250067)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Param = {\n",
    "    'host'          : 'tdprd2.td.teradata.com', \n",
    "    'user'          : 'dm250067', \n",
    "    'password'      : \"ENCRYPTED_PASSWORD(file:{},file:{})\".format ('../../PassKey.properties','../../EncPass.properties'), #getpass.getpass(), \n",
    "    'logmech'       : 'LDAP',\n",
    "    'database'      : 'ADLSLSEMEA_DEMO_BANKING',\n",
    "    'temp_database_name' : 'dm250067'\n",
    "    }\n",
    "\n",
    "Param = {\n",
    "    'host'          : 'tdprd3.td.teradata.com', \n",
    "    'user'          : 'dm250067', \n",
    "    'password'      : \"ENCRYPTED_PASSWORD(file:{},file:{})\".format ('../../PassKey.properties','../../EncPass.properties'), #getpass.getpass(), \n",
    "    'logmech'       : 'LDAP',\n",
    "    'database'      : 'ADLDSD_CHURN',\n",
    "    'temp_database_name' : 'dm250067'\n",
    "    }\n",
    "\n",
    "tdml.create_context(**Param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4c1a8-1ee3-4ea8-a3fa-1f0c3e0809cb",
   "metadata": {},
   "source": [
    "## 2 - Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20cccfd-b426-4f17-aa6e-2a3701b3e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:/Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb/model_modules/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"$model_local_path/model_modules/training.py\"\n",
    "from teradataml import (\n",
    "    DataFrame,\n",
    "    OneHotEncodingFit,\n",
    "    OneHotEncodingTransform,\n",
    "    ScaleFit,\n",
    "    ScaleTransform,\n",
    "    DecisionForest,\n",
    "    configure\n",
    ")\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "configure.val_install_location = 'TRNG_XSP'\n",
    "\n",
    "def plot_roc_curve(fi, img_filename):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    feat_importances = pd.Series(fi)\n",
    "    feat_importances.nlargest(10).plot(kind='barh').set_title('Feature Importance')\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(img_filename, dpi=500)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name   = context.dataset_info.target_names[0]\n",
    "    entity_key    = context.dataset_info.entity_key\n",
    "\n",
    "    # read training dataset from Teradata and convert to pandas\n",
    "    train_df      = DataFrame.from_query(context.dataset_info.sql)\n",
    "    \n",
    "    if 'type' in feature_names:\n",
    "        print (\"OneHotEncoding using InDB Functions...\")\n",
    "        \n",
    "        transaction_types = list(train_df[['type','txn_id']].groupby(['type']).count().to_pandas()['type'].values)\n",
    "\n",
    "\n",
    "        onehot = OneHotEncodingFit(data           = train_df,\n",
    "                                        is_input_dense  = True,\n",
    "                                        target_column      = '\"type\"',\n",
    "                                        categorical_values = transaction_types,\n",
    "                                        other_column=\"other\"\n",
    "                                       )\n",
    "\n",
    "        train_df_onehot = OneHotEncodingTransform(data=train_df,\n",
    "                                           object=onehot.result,\n",
    "                                           is_input_dense=True\n",
    "                                          ).result\n",
    "\n",
    "        onehot.result.to_sql(f\"onehot_${context.model_version}\", if_exists=\"replace\")\n",
    "        print(\"Saved onehot\")\n",
    "        \n",
    "        feature_names_after_one_hot = [c for c in feature_names if c != 'type'] + ['type_'+c for c in transaction_types]\n",
    "        category_features = ['type_'+c for c in transaction_types]\n",
    "    else:\n",
    "        train_df_onehot = train_df\n",
    "        feature_names_after_one_hot = feature_names\n",
    "        category_features = []\n",
    "    \n",
    "    print (\"Scaling using InDB Functions...\")\n",
    "    \n",
    "    scaler = ScaleFit(\n",
    "        data           = train_df_onehot,\n",
    "        target_columns = feature_names_after_one_hot,\n",
    "        scale_method   = context.hyperparams[\"scale_method\"],\n",
    "        miss_value     = context.hyperparams[\"miss_value\"],\n",
    "        global_scale   = context.hyperparams[\"global_scale\"].lower() in ['true', '1'],\n",
    "        multiplier     = context.hyperparams[\"multiplier\"],\n",
    "        intercept      = context.hyperparams[\"intercept\"]\n",
    "    )\n",
    "\n",
    "    scaled_train = ScaleTransform(\n",
    "        data           = train_df_onehot,\n",
    "        object         = scaler.output,\n",
    "        accumulate     = [target_name, entity_key]\n",
    "    ).result\n",
    "    \n",
    "    scaler.output.to_sql(f\"scaler_${context.model_version}\", if_exists=\"replace\")\n",
    "    print(\"Saved scaler\")\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    model = DecisionForest(\n",
    "        input_columns        = feature_names_after_one_hot,\n",
    "        response_column      = target_name,\n",
    "        data                 = scaled_train,\n",
    "        max_depth            = context.hyperparams[\"max_depth\"],\n",
    "        num_trees            = context.hyperparams[\"num_trees\"],\n",
    "        min_node_size        = context.hyperparams[\"min_node_size\"],\n",
    "        mtry                 = context.hyperparams[\"mtry\"],\n",
    "        mtry_seed            = context.hyperparams[\"mtry_seed\"],\n",
    "        seed                 = context.hyperparams[\"seed\"],\n",
    "        tree_type            = 'CLASSIFICATION'\n",
    "    )\n",
    "    \n",
    "    model.result.to_sql(f\"model_${context.model_version}\", if_exists=\"replace\")    \n",
    "    print(\"Saved trained model\")\n",
    "\n",
    "    record_training_stats(\n",
    "        train_df_onehot,\n",
    "        features    = feature_names_after_one_hot,\n",
    "        targets     = [target_name],\n",
    "        categorical = [target_name]+category_features,\n",
    "        feature_importance = {f:0 for f in feature_names_after_one_hot},\n",
    "        context     = context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9450907a-94ba-4618-bc00-4b5448ec8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function record_training_stats in module aoa.stats.stats:\n",
      "\n",
      "record_training_stats(df: teradataml.dataframe.dataframe.DataFrame, features: List[str], targets: List[str] = [], categorical: List[str] = [], context: aoa.context.model_context.ModelContext = {}, feature_importance: Dict[str, float] = {}, **kwargs) -> Dict\n",
      "    Compute and record the dataset statistics used for training. This information provides ModelOps with a snapshot\n",
      "    of the dataset at this point in time (i.e. at the point of training). ModelOps uses this information for data and\n",
      "    prediction drift monitoring. It can also be used for data quality monitoring as all of the information which is\n",
      "    captured here is available to configure an alert on (e.g. max > some_threshold).\n",
      "    \n",
      "    Depending on the type of variable (categorical or continuous), different statistics and distributions are computed.\n",
      "    All of this is computed in Vantage via the Vantage Analytics Library (VAL).\n",
      "    \n",
      "    Continuous Variable:\n",
      "        Distribution: Histogram\n",
      "        Statistics: Min, Max, Average, Skew, etc, nulls\n",
      "    \n",
      "    Categorical Variable:\n",
      "        Distribution: Frequency\n",
      "        Statistics: nulls\n",
      "    \n",
      "    The following example shows how you would use this function for a binary classification problem where the there\n",
      "    are 3 features and 1 target. As it is classification, the target must be categorical and in this case, the features\n",
      "    are all continuous.\n",
      "    example usage:\n",
      "        training_df = DataFrame.from_query(\"SELECT * from my_table\")\n",
      "    \n",
      "        record_training_stats(training_df,\n",
      "                              features=[\"feat1\", \"feat2\", \"feat3\"],\n",
      "                              targets=[\"targ1\"],\n",
      "                              categorical=[\"targ1\"],\n",
      "                              context=context)\n",
      "    \n",
      "    :param df: teradataml dataframe used for training with the feature and target variables\n",
      "    :type df: teradataml.DataFrame\n",
      "    :param features: feature variable(s) used in this training\n",
      "    :type features: List[str]\n",
      "    :param targets: target variable(s) used in this training\n",
      "    :type targets: List[str]\n",
      "    :param categorical: variable(s) (feature or target) that is categorical\n",
      "    :type categorical: List[str]\n",
      "    :param context: ModelContext which is associated with that training invocation\n",
      "    :type context: ModelContext\n",
      "    :param feature_importance: (Optional) feature importance\n",
      "    :type feature_importance: Dict[str, float]\n",
      "    :return: the computed data statistics\n",
      "    :rtype: Dict\n",
      "    :raise ValueError: if features or targets are not provided\n",
      "    :raise TypeError: if df is not of type teradataml.DataFrame\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(record_training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3740ac-b6f1-4c07-8325-a340b1249efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoding using InDB Functions...\n",
      "Saved onehot\n",
      "Scaling using InDB Functions...\n",
      "Saved scaler\n",
      "Starting training...\n",
      "Saved trained model\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Ensure feature statistics metadata in ADLDSD_CHURN.aoa_statistics_metadata are up to date. Attempted to compute stats for ['newbalanceorig', 'newbalancedest', 'oldbalanceorig', 'amount', 'oldbalancedest'] but only found the following continuous variables dict_keys([]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_local_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users/dm250067/OneDrive - Teradata/Documents/01 - Code Development/modelops-demo-models/model_definitions/transaction_fraud_indb/model_modules\\training.py:106\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(context, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;241m.\u001b[39mmodel_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved trained model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mrecord_training_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df_onehot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_names_after_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mcategory_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature_names_after_one_hot\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\aoa\\stats\\stats.py:95\u001b[0m, in \u001b[0;36mrecord_training_stats\u001b[1;34m(df, features, targets, categorical, context, feature_importance, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     feature_metadata_group \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mdataset_info\u001b[38;5;241m.\u001b[39mfeature_metadata_monitoring_group\n\u001b[0;32m     93\u001b[0m     data_stats_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(context\u001b[38;5;241m.\u001b[39martifact_output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_stats.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m data_stats \u001b[38;5;241m=\u001b[39m \u001b[43m_capture_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_importance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeature_metadata_fqtn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_metadata_fqtn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfeature_metadata_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_metadata_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_stats_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    104\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data_stats, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m_NpEncoder)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\vantage39\\lib\\site-packages\\aoa\\stats\\stats_util.py:338\u001b[0m, in \u001b[0;36m_capture_stats\u001b[1;34m(df, features, targets, categorical, feature_importance, feature_metadata_fqtn, feature_metadata_group)\u001b[0m\n\u001b[0;32m    335\u001b[0m stats_metadata \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_feature_stats(feature_metadata_fqtn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x \u001b[38;5;129;01min\u001b[39;00m stats_metadata\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m continuous_vars):\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsure feature statistics metadata in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_metadata_fqtn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are up to date. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to compute stats for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontinuous_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but only found the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats_metadata\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    342\u001b[0m reference_edges \u001b[38;5;241m=\u001b[39m [stats_metadata[v\u001b[38;5;241m.\u001b[39mlower()][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m continuous_vars]\n\u001b[0;32m    344\u001b[0m histogram \u001b[38;5;241m=\u001b[39m valib\u001b[38;5;241m.\u001b[39mHistogram(data\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m    345\u001b[0m                             columns\u001b[38;5;241m=\u001b[39mcontinuous_vars,\n\u001b[0;32m    346\u001b[0m                             boundaries\u001b[38;5;241m=\u001b[39m_convert_all_edges_to_val_str(reference_edges))\n",
      "\u001b[1;31mException\u001b[0m: Ensure feature statistics metadata in ADLDSD_CHURN.aoa_statistics_metadata are up to date. Attempted to compute stats for ['newbalanceorig', 'newbalancedest', 'oldbalanceorig', 'amount', 'oldbalancedest'] but only found the following continuous variables dict_keys([])."
     ]
    }
   ],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the training dataset \n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM {Param['database']}.transactions\n",
    "where fold = 'train'\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": Param['database'],\n",
    "    \"table\": \"aoa_statistics_metadata\"\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    # scaler\n",
    "    \"scale_method\":\"STD\",\n",
    "    \"miss_value\":\"KEEP\",\n",
    "    \"global_scale\":\"False\",\n",
    "    \"multiplier\":\"1\",\n",
    "    \"intercept\":\"0\",\n",
    "    # decision forest\n",
    "    \"max_depth\": 15, \n",
    "    \"num_trees\": 72,\n",
    "    \"min_node_size\": 1,\n",
    "    \"mtry\": 6,\n",
    "    \"mtry_seed\": 1,\n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "entity_key    = \"txn_id\"\n",
    "target_names  = [\"isFraud\"]\n",
    "feature_names = ['amount', 'oldbalanceOrig', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'type']\n",
    "\n",
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "dataset_info = DatasetInfo(\n",
    "    sql=sql,\n",
    "    entity_key=entity_key,\n",
    "    feature_names=feature_names,\n",
    "    target_names=target_names,\n",
    "    feature_metadata=feature_metadata\n",
    ")\n",
    "\n",
    "ctx = ModelContext(\n",
    "    hyperparams=hyperparams,\n",
    "    dataset_info=dataset_info,\n",
    "    artifact_output_path=f'{model_local_path}/model_modules/artifacts/',\n",
    "    model_version=\"InDB_v1\",\n",
    "    model_table=\"aoa_model_indb_v1\"\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.expanduser(f\"{model_local_path}/model_modules\"))\n",
    "import training\n",
    "training.train(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c652f-c6da-4786-8d8f-90c4d3519f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.db_list_tables(schema_name='ADLDSD_CHURN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28bf29-12a0-4713-a531-c259e914a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.execute_sql(\"SEL * FROM ADLDSD_CHURN.aoa_statistics_metadata WHERE column_type='continuous'\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d138c-0269-405e-aede-ac5ec493b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69bdb7-ff2a-4b63-8b72-40609456d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.read_sql(\"SEL * FROM ADLDSD_CHURN.aoa_statistics_metadata WHERE column_type='continuous'\", tdml.get_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fc7a8-4a9a-47dc-a5ec-48eecedf8800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
